{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "from transformers import GPTNeoXConfig, GPTNeoXForCausalLM\n",
    "import tqdm\n",
    "\n",
    "from result_funcs import get_result_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_df(result_path):\n",
    "    result_df = pd.read_parquet(result_path, columns=['result', 'args'])\n",
    "    result_df['return'] = result_df['result'].map(lambda r: r['return'])\n",
    "    result_df['id'] = result_df['return'].map(lambda r: r['id'] if r and 'id' in r else None)\n",
    "    video_df = result_df[result_df['id'].map(lambda i: i is not None)]\n",
    "    \n",
    "    return video_df\n",
    "\n",
    "def parse_ids(df):\n",
    "    df = df.sort_values('id')\n",
    "    df['bits'] = df['id'].map(lambda i: format(int(i), '064b'))\n",
    "    # only looking at 1 sequence ID system for now\n",
    "    df = df[df['bits'].map(lambda b: b[50:56] == '001101')]\n",
    "    df['bits'] = df['bits'].map(lambda bits: np.array([int(b) for b in bits]))\n",
    "\n",
    "    # Example IDs\n",
    "    ids = df['bits'].tolist()\n",
    "    return ids\n",
    "\n",
    "def load_data(num_files=None):\n",
    "    data_dir_path = os.path.join('/', 'mnt', 'bigone', 'bsteel', 'tiktok', 'data')\n",
    "    result_paths = list(get_result_paths(data_dir_path))\n",
    "    result_paths = sorted(result_paths)\n",
    "\n",
    "    seconds_ids = []\n",
    "    if num_files:\n",
    "        result_paths = result_paths[:num_files]\n",
    "    for result_path in tqdm.tqdm(result_paths):\n",
    "        video_path = result_path.replace('results.parquet.gzip', 'videos.parquet.gzip')\n",
    "        if not os.path.exists(video_path):\n",
    "            batch_df = get_video_df(result_path)\n",
    "            batch_df.to_parquet(video_path, compression='gzip')\n",
    "        else:\n",
    "            batch_df = pd.read_parquet(video_path)\n",
    "\n",
    "        seconds_ids.extend(parse_ids(batch_df))\n",
    "\n",
    "    \n",
    "    return seconds_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_ids = load_data(num_files=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDDataset(Dataset):\n",
    "    def __init__(self, ids, sequence_length=10):\n",
    "        self.ids = [self.binary_to_tensor(id) for id in ids]\n",
    "        self.seq_len = sequence_length\n",
    "\n",
    "    def binary_to_tensor(self, binary_id):\n",
    "        return torch.tensor([int(bit) for bit in binary_id], dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.stack(self.ids[index:index+self.seq_len]),\n",
    "            torch.stack(self.ids[index+1:index+1+self.seq_len])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticSequenceWorkerDataset(Dataset):\n",
    "    def __init__(self, sequence_length=10):\n",
    "        num_ids = 10000\n",
    "        sequence_ids = np.zeros(num_ids, dtype=int)\n",
    "        worker_ids = np.zeros(num_ids, dtype=int)\n",
    "        for i in range(num_ids):\n",
    "            sequence_id = int(i % 10)\n",
    "            worker_id = int((i // 10) % 10)\n",
    "            sequence_ids[i] = sequence_id\n",
    "            worker_ids[i] = worker_id\n",
    "        self._convert_to_id_seq(sequence_ids, worker_ids, sequence_length)\n",
    "\n",
    "    def _convert_to_id_seq(self, sequence_ids, worker_ids, sequence_length):\n",
    "        unique_worker_ids = np.unique(worker_ids)\n",
    "        max_sequence_id = sequence_ids.max()\n",
    "        worker_id_map = {worker_id: i for worker_id, i in zip(unique_worker_ids, range(max_sequence_id + 1, max_sequence_id + 1 + len(unique_worker_ids)))}\n",
    "        worker_ids = np.array([worker_id_map[w] for w in worker_ids])\n",
    "        self.seq = np.stack([sequence_ids, worker_ids])\n",
    "        self.seq = einops.rearrange(self.seq, 't l -> (l t)')\n",
    "        \n",
    "        self.seq_len = sequence_length\n",
    "        self.vocab_size = sequence_ids.max() + 1 + len(unique_worker_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.seq[index:index+self.seq_len], dtype=torch.int64)\n",
    "        )\n",
    "\n",
    "class SequenceWorkerDataset(SyntheticSequenceWorkerDataset):\n",
    "    def __init__(self, seconds_ids, sequence_length=10):\n",
    "        self.num_seconds = len(seconds_ids)\n",
    "        ids = [id for second in seconds_ids for id in second]\n",
    "        id_bits = np.array(ids)\n",
    "        sequence_bits = id_bits[:, 42:50]\n",
    "        worker_bits = id_bits[:, 56:]\n",
    "        sequence_ids = sequence_bits.dot(1 << np.arange(sequence_bits.shape[-1] - 1, -1, -1))\n",
    "        worker_ids = worker_bits.dot(1 << np.arange(worker_bits.shape[-1] - 1, -1, -1))\n",
    "        self._convert_to_id_seq(sequence_ids, worker_ids, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(seconds_ids, dataset_cls, sequence_length=None):\n",
    "    train_size = int(0.8 * len(seconds_ids))\n",
    "    train_ids = seconds_ids[:train_size]\n",
    "    test_ids = seconds_ids[train_size:]\n",
    "    train_dataset = dataset_cls(train_ids, sequence_length)\n",
    "    test_dataset = dataset_cls(test_ids, sequence_length)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def get_loaders(train_dataset, test_dataset):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "seq_len = 2048\n",
    "method = 'sequence'\n",
    "if method == 'bits':\n",
    "    train_dataset, test_dataset = create_datasets(seconds_ids, IDDataset, sequence_length=seq_len)\n",
    "    train_loader, test_loader = get_loaders(train_dataset, test_dataset)\n",
    "\n",
    "    # Assuming each ID has 64 bits\n",
    "    config = GPTNeoXConfig(\n",
    "        vocab_size=64,  # output a 64 bit string\n",
    "        max_position_embeddings=seq_len,  # Maximum sequence length\n",
    "        hidden_size=64,  # Embedding size\n",
    "        intermediate_size=8,\n",
    "        num_hidden_layers=1,  # Number of transformer layers\n",
    "        num_attention_heads=1,  # Number of attention heads\n",
    "        # torch_dtype=torch.float16,\n",
    "    )\n",
    "elif method == 'sequence':\n",
    "    train_dataset, test_dataset = create_datasets(seconds_ids, SequenceWorkerDataset, sequence_length=seq_len)\n",
    "    train_loader, test_loader = get_loaders(train_dataset, test_dataset)\n",
    "\n",
    "    # Assuming each ID has 64 bits\n",
    "    config = GPTNeoXConfig(\n",
    "        vocab_size=train_dataset.vocab_size,  # output a 64 bit string\n",
    "        max_position_embeddings=seq_len,  # Maximum sequence length\n",
    "        hidden_size=16,  # Embedding size\n",
    "        intermediate_size=16,\n",
    "        num_hidden_layers=4,  # Number of transformer layers\n",
    "        num_attention_heads=2,  # Number of attention heads\n",
    "        # torch_dtype=torch.float16,\n",
    "    )\n",
    "elif method == 'synthetic':\n",
    "    train_dataset, test_dataset = create_datasets(seconds_ids, SyntheticSequenceWorkerDataset, sequence_length=seq_len)\n",
    "    train_loader, test_loader = get_loaders(train_dataset, test_dataset)\n",
    "\n",
    "    # Assuming each ID has 64 bits\n",
    "    config = GPTNeoXConfig(\n",
    "        vocab_size=train_dataset.vocab_size,  # output a 64 bit string\n",
    "        max_position_embeddings=seq_len,  # Maximum sequence length\n",
    "        hidden_size=16,  # Embedding size\n",
    "        intermediate_size=16,\n",
    "        num_hidden_layers=4,  # Number of transformer layers\n",
    "        num_attention_heads=2,  # Number of attention heads\n",
    "        # torch_dtype=torch.float16,\n",
    "    )\n",
    "    \n",
    "model = GPTNeoXForCausalLM(config)\n",
    "# model = model.half()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, device, epochs=5):\n",
    "    model.train()\n",
    "    optimizer = transformers.AdamW(model.parameters(), lr=0.01)\n",
    "    scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=int(epochs * 0.1), num_training_steps=epochs)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    method = 'sequence'\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        current_loss = 0\n",
    "        pbar = tqdm.tqdm(loader)\n",
    "        for batch in pbar:\n",
    "            if method == 'bits':\n",
    "                input_ids, labels = batch\n",
    "                input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "                outputs = model(inputs_embeds=input_ids)\n",
    "                loss = loss_fn(outputs.logits, labels)\n",
    "            elif method == 'sequence':\n",
    "                input_ids = batch\n",
    "                input_ids = input_ids.to(device)\n",
    "                outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "                loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            current_loss = loss.item() / (input_ids.size(0) * input_ids.size(1))\n",
    "            total_loss += current_loss\n",
    "            if method == 'bits':\n",
    "                pbar.set_description(f\"Epoch {epoch+1}, Loss: {current_loss:.4f}\")\n",
    "            elif method == 'sequence':\n",
    "                pbar.set_description(f\"Epoch {epoch+1}, Loss: {current_loss:.8f}\")\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, test_loader, device):\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids in tqdm.tqdm(test_loader):\n",
    "            input_ids = input_ids.to(device)\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            labels = input_ids[:, 1:]\n",
    "            predictions = predictions[:, :-1]\n",
    "            num_correct += (predictions == labels).sum().item()\n",
    "            num_total += input_ids.size(0) * input_ids.size(1)\n",
    "    print(f'Accuracy: {num_correct / num_total}')\n",
    "\n",
    "def evaluate_usage(model, test_dataset, device):\n",
    "    model.eval()\n",
    "    num_requests = 0\n",
    "    num_misses = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.tqdm(range(len(test_dataset))):\n",
    "            # TODO sample sequence and worker ID from the model\n",
    "            # test it (against test dataset)\n",
    "            # if in the test dataset, add it to the sequence so far\n",
    "            # else, add it to failed IDs, and generate new ID excluding failed IDs\n",
    "            input_ids = input_ids.to(device)\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            labels = input_ids[:, 1:]\n",
    "            predictions = predictions[:, :-1]\n",
    "            num_correct += (predictions == labels).sum().item()\n",
    "            num_total += input_ids.size(0) * input_ids.size(1)\n",
    "    print(f'Accuracy: {num_correct / num_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader, device, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(test_dataset):\n",
    "    with open(os.path.join('..', 'figs', 'all_videos', 'all_two_segments_combinations.json'), 'r') as file:\n",
    "        data = json.load(file)\n",
    "    num_reqs_per_milli = len(data)\n",
    "    num_seconds = test_dataset.num_seconds\n",
    "    print(f\"Num Requests: {num_reqs_per_milli * 1000 * num_seconds}, Coverage: {1.0}\")\n",
    "    \n",
    "    \n",
    "baseline(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whatforwhere",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
